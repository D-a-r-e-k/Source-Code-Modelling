{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Source code modelling LSTM.ipynb","version":"0.3.2","provenance":[{"file_id":"1FkzWfaPyqDyy3DqG-a8KuTCSBYkXZGB5","timestamp":1554738134388},{"file_id":"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Exercise.ipynb","timestamp":1543771998870}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"l4tACWi_93yA","colab_type":"text"},"source":["# Source code modelling using LSTM\n","\n"]},{"cell_type":"code","metadata":{"id":"DstDFRlg_B-S","colab_type":"code","outputId":"609dd6e1-67ef-4621-8851-db5dbb49aa63","executionInfo":{"status":"ok","timestamp":1556351920370,"user_tz":-180,"elapsed":31197,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["# Install a Drive FUSE wrapper.\n","# https://github.com/astrada/google-drive-ocamlfuse\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 131304 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1X6ERQBBAK-R","colab_type":"code","outputId":"fcbb9f30-9565-45b9-92dd-8da99440d8e1","executionInfo":{"status":"ok","timestamp":1556351976281,"user_tz":-180,"elapsed":51657,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["from google.colab import auth\n","auth.authenticate_user()\n","\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mNAU03VgFdtR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvQkMHioCFFh","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bBPvW7KICs_A","colab_type":"code","outputId":"2b98f2bf-31aa-4a50-fb12-22332239df6a","executionInfo":{"status":"ok","timestamp":1556351994218,"user_tz":-180,"elapsed":6969,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["\n","!pip install torch\n","!pip install torchvision"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.1.post2)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6sneUpV193yM","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3K45T6k-_52","colab_type":"code","outputId":"b3bd28ea-b301-43bb-d61e-a58aca23b7df","executionInfo":{"status":"ok","timestamp":1556352009862,"user_tz":-180,"elapsed":3808,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["\n","os.listdir(\"drive/datasets\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['flower_data',\n"," 'anna.txt',\n"," 'cat_to_name.json',\n"," 'checkpoint.pth',\n"," 'checkpoint_classifier.pth',\n"," 'intermediate_weights.csv',\n"," 'smells_train.csv',\n"," 'smells_valid.csv',\n"," 'smells_test.csv',\n"," 'code_corpus.csv',\n"," 'dictionary.csv',\n"," 'dictionary_large.csv',\n"," 'code_corpus_large.csv',\n"," 'SCRNN.net',\n"," 'smells-dataset']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"J-L1uedn93yW","colab_type":"text"},"source":["## Load in Data\n","\n"]},{"cell_type":"code","metadata":{"id":"lTD8rMPr93yY","colab_type":"code","colab":{}},"source":["with open('drive/datasets/dictionary_large.csv', 'r') as f:\n","    text = f.read()\n","    dictionary = text.split('\\n')\n","    dictionary = list(filter(None, dictionary))\n","\n","with open('drive/datasets/code_corpus_large.csv', 'r') as f:\n","    text = f.read()\n","    methods = text.split('\\n')\n","    \n","    methods = list(filter(None, methods))\n","    for i in range(len(methods)):\n","      methods[i] = ','.join(list(filter(None, methods[i].split(','))))\n","    \n","dictionary.append('<unk>')\n","dictionary.append('<num>')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_2x02vS93yg","colab_type":"code","outputId":"9f78ff30-03f5-4804-fcb8-9114d42d44c6","executionInfo":{"status":"ok","timestamp":1556097580810,"user_tz":-180,"elapsed":561,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(len(dictionary))\n","print(len(methods))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1001\n","26039\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GoLzTjk693yo","colab_type":"text"},"source":["### Encoding using dictionary\n"]},{"cell_type":"code","metadata":{"id":"MD95PEPD93yq","colab_type":"code","colab":{}},"source":["int2token = dict(enumerate(dictionary))\n","token2int = {ch: ii for ii, ch in int2token.items()}\n","\n","#encoded = np.array([token2int[ch] for ch in text])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJtx07San_0t","colab_type":"code","colab":{}},"source":["methods = methods[:-1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zS_TBtC_9z2n","colab_type":"code","colab":{}},"source":["encoded_methods = [[token2int[el] for el in method.split(',')] for method in methods]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NXy7OT-XJN-k","colab_type":"code","colab":{}},"source":["unknown_token = token2int['<unk>']\n","\n","batch_size = 50\n","sequence_length = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-8sWgGeL9of","colab_type":"code","outputId":"c217e2fb-51b5-4e25-cb35-218c8ffac362","executionInfo":{"status":"ok","timestamp":1556097598839,"user_tz":-180,"elapsed":581,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(encoded_methods[0])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["530"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"CsosLdrX93yx","colab_type":"code","outputId":"1a72f776-f1f7-491a-dda1-ef2ce77184de","executionInfo":{"status":"ok","timestamp":1556097604475,"user_tz":-180,"elapsed":1517,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["encoded_methods_all = []\n","for i, el in enumerate(encoded_methods):\n","  if len(el) < sequence_length:\n","    encoded_methods[i].extend([unknown_token] * (sequence_length - len(el)))\n","    \n","  enough = int(len(encoded_methods[i]) / sequence_length)\n","  encoded_methods_all.extend(encoded_methods[i][:enough * sequence_length])\n","\n","print(len(encoded_methods_all))\n","\n","print(encoded_methods_all[:5])\n","\n","  \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["34462100\n","[973, 3, 55, 56, 999]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jFLkh1jv93zX","colab_type":"code","colab":{}},"source":["def get_batches(arr, batch_size, seq_length):\n","\n","    n_batches = int(np.floor(arr.size / (batch_size * seq_length)))\n","\n","    arr = arr[:n_batches * (batch_size * seq_length)]\n","    arr = arr.reshape(batch_size, -1)\n","\n","    for n in range(0, arr.shape[1], seq_length):\n","\n","        x = arr[:,n:n+seq_length]\n","\n","        if n+seq_length == arr.shape[1]:\n","          y = np.zeros_like(x)\n","          y[:,:seq_length - 1] = arr[:,n+1:n+seq_length]\n","          y[:,seq_length - 1] = arr[:,0]\n","        else:\n","          y = arr[:,n+1:n+seq_length+1]\n","        yield x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBo-D_Ud93zf","colab_type":"code","colab":{}},"source":["encoded_methods_array = np.array(encoded_methods_all)\n","\n","batches = get_batches(encoded_methods_array, 50, 100)\n","x, y = next(batches)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueZ9Vn-4EKDt","colab_type":"code","outputId":"987151c8-4b45-4624-c0b5-d9c1c96e5165","executionInfo":{"status":"ok","timestamp":1555776106556,"user_tz":-180,"elapsed":1184,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["print(x.shape)\n","print(y.shape)\n","print(x[:5, :5])\n","print(y[:5, :5])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(50, 100)\n","(50, 100)\n","[[977   5 883  25  27]\n"," [ 51 451  49 991 848]\n"," [999  25 708 186  27]\n"," [977   5 113  25  27]\n"," [977   5 113  25  27]]\n","[[  5 883  25  27 108]\n"," [451  49 991 848  51]\n"," [ 25 708 186  27  49]\n"," [  5 113  25  27 108]\n"," [  5 113  25  27 108]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fMOWsf5t93zr","colab_type":"text"},"source":["### LSTM Neural Network Definition\n"]},{"cell_type":"code","metadata":{"id":"mu9lH0SK93zu","colab_type":"code","outputId":"6eef25fe-1a1a-4504-ed8d-167ae60c09c9","executionInfo":{"status":"ok","timestamp":1556097621821,"user_tz":-180,"elapsed":636,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","train_on_gpu =  True\n","if(train_on_gpu):\n","    print('Training on GPU!')\n","else: \n","    print('No GPU available, training on CPU; consider making n_epochs very small.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training on GPU!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"47MaQ4-a93zy","colab_type":"code","colab":{}},"source":["class SCRNN(nn.Module):\n","    \n","    def __init__(self, tokens, embedding_dim = 50, n_hidden=50, n_layers=2,\n","                               drop_prob=0.7, lr=0.001, dictionary_size=5001):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","        self.lr = lr\n","        \n","        self.embedding = nn.Embedding(len(tokens), embedding_dim)\n","        \n","        self.lstm = nn.LSTM(embedding_dim, n_hidden, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","        \n","        self.dropout = nn.Dropout(drop_prob)\n","        \n","        self.fc = nn.Linear(n_hidden, len(tokens))\n","        \n","        \n","    \n","    def forward(self, x, hidden):\n","\n","        embedding = self.embedding(x)\n","\n","        out_r, hidden = self.lstm(embedding, hidden)\n","        \n","        out_features = out_r.reshape([out_r.shape[0] * out_r.shape[1], self.n_hidden])\n","        \n","        out = self.dropout(out_r)\n","\n","        out = out.reshape([out.shape[0] * out.shape[1], self.n_hidden])\n","        \n","        out = self.fc(out)\n","        \n","        return out, hidden, out_features\n","    \n","    \n","    def init_hidden(self, batch_size):\n","\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        \n","        return hidden\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lxhEUL793z1","colab_type":"text"},"source":["## Training\n"]},{"cell_type":"code","metadata":{"id":"DMO0eCRG93z2","colab_type":"code","colab":{}},"source":["def train(net, train_data, val_data, epochs=10, batch_size=batch_size, seq_length=sequence_length, lr=0.001, clip=5, print_every=10):\n","\n","    best_loss = 100000\n","    \n","    net.train()\n","    \n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    if(train_on_gpu):\n","        net.cuda()\n","    \n","    counter = 0\n","    for e in range(epochs):\n","\n","        h = net.init_hidden(batch_size)\n","        \n","        for x, y in get_batches(train_data, batch_size, seq_length):\n","            \n","            counter += 1\n","            \n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","            \n","            if(train_on_gpu):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","\n","            net.zero_grad()\n","\n","            output, h, _ = net(inputs, h)\n","            \n","            loss = criterion(output, targets.contiguous().view(batch_size*seq_length))\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            opt.step()\n","            \n","            if counter % print_every == 0:\n","                val_h = net.init_hidden(batch_size)\n","                val_losses = []\n","                net.eval()\n","                for x, y in get_batches(val_data, batch_size, seq_length):\n","\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","                    \n","                    val_h = tuple([each.data for each in val_h])\n","                    \n","                    inputs, targets = x, y\n","                    if(train_on_gpu):\n","                        inputs, targets = inputs.cuda(), targets.cuda()\n","\n","                    output, val_h, _ = net(inputs, val_h)\n","\n","                    val_loss = criterion(output, targets.contiguous().view(batch_size*seq_length))\n","                \n","                    val_losses.append(val_loss.item())\n","                \n","                net.train() \n","                \n","                val_loss = np.mean(val_losses)\n","                if val_loss < best_loss:\n","                  save_model(net)\n","                  best_loss = val_loss\n","                  print('Persisted the best model.')\n","                \n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.item()),\n","                      \"Val Loss: {:.4f}\".format(val_loss),\n","                      \"Perplexity: {:.4f}\".format(np.exp(val_loss)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCZVEFzw93z9","colab_type":"code","outputId":"1b885f09-c441-47c8-c83e-ff478c3f3348","executionInfo":{"status":"ok","timestamp":1556098477918,"user_tz":-180,"elapsed":893,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":155}},"source":["\n","n_hidden= 512\n","n_layers= 2\n","embedding_dim = 50\n","\n","net = SCRNN(dictionary, embedding_dim, n_hidden, n_layers)\n","print(net)\n","\n","print('Number of parameters: ')\n","np.sum([np.prod(x.shape) for x in net.parameters()])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["SCRNN(\n","  (embedding): Embedding(1001, 50)\n","  (lstm): LSTM(50, 512, num_layers=2, batch_first=True, dropout=0.7)\n","  (dropout): Dropout(p=0.7)\n","  (fc): Linear(in_features=512, out_features=1001, bias=True)\n",")\n","Number of parameters: \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["3819883"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"markdown","metadata":{"id":"SsLMxecZ930A","colab_type":"text"},"source":["### Set your training hyperparameters!"]},{"cell_type":"code","metadata":{"id":"M5I_9q9KIEHt","colab_type":"code","outputId":"978130d6-84b6-45f3-d87c-4a1336f7d731","executionInfo":{"status":"ok","timestamp":1556097649786,"user_tz":-180,"elapsed":6065,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["import numpy as np\n","\n","val_frac = 0.1\n","\n","encoded_methods_array = np.array(encoded_methods_all)\n","\n","seqs = int(encoded_methods_array.shape[0] / sequence_length)\n","\n","sep_point = int(seqs * (1 - val_frac)) * sequence_length\n","train_methods, val_methods = encoded_methods_array[:sep_point], encoded_methods_array[sep_point:]\n","\n","unknown_token_id = token2int['<unk>']\n","\n","train_tokens = set(train_methods)\n","val_tokens = set(val_methods)\n","\n","print(len(train_tokens))\n","print(len(val_tokens))\n","\n","unknown_tokens = val_tokens - train_tokens\n","\n","print(len(unknown_tokens))\n","\n","val_methods = np.array([token if token not in unknown_tokens else unknown_token_id for token in val_methods])\n","\n","print(len(set(val_methods))) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["983\n","612\n","17\n","595\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"PDeWfOmh930B","colab_type":"code","outputId":"bf71f506-743b-4a74-a00e-2a65d2e270fd","executionInfo":{"status":"ok","timestamp":1556104140412,"user_tz":-180,"elapsed":3151933,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":7612}},"source":["CUDA_LAUNCH_BLOCKING=1\n","n_epochs = 3\n","\n","train(net, train_methods, val_methods, epochs=n_epochs, batch_size=batch_size, seq_length=sequence_length, lr=0.001, print_every=50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Persisted the best model.\n","Epoch: 1/3... Step: 50... Loss: 3.8200... Val Loss: 3.8854 Perplexity: 48.6861\n","Persisted the best model.\n","Epoch: 1/3... Step: 100... Loss: 3.1288... Val Loss: 3.2197 Perplexity: 25.0211\n","Persisted the best model.\n","Epoch: 1/3... Step: 150... Loss: 2.7573... Val Loss: 2.9215 Perplexity: 18.5700\n","Persisted the best model.\n","Epoch: 1/3... Step: 200... Loss: 2.4581... Val Loss: 2.7650 Perplexity: 15.8786\n","Persisted the best model.\n","Epoch: 1/3... Step: 250... Loss: 2.3514... Val Loss: 2.6477 Perplexity: 14.1216\n","Persisted the best model.\n","Epoch: 1/3... Step: 300... Loss: 2.2373... Val Loss: 2.5657 Perplexity: 13.0104\n","Persisted the best model.\n","Epoch: 1/3... Step: 350... Loss: 2.2097... Val Loss: 2.4719 Perplexity: 11.8450\n","Persisted the best model.\n","Epoch: 1/3... Step: 400... Loss: 2.1161... Val Loss: 2.4204 Perplexity: 11.2508\n","Persisted the best model.\n","Epoch: 1/3... Step: 450... Loss: 1.9679... Val Loss: 2.3663 Perplexity: 10.6580\n","Persisted the best model.\n","Epoch: 1/3... Step: 500... Loss: 2.1288... Val Loss: 2.3164 Perplexity: 10.1388\n","Persisted the best model.\n","Epoch: 1/3... Step: 550... Loss: 1.9630... Val Loss: 2.2557 Perplexity: 9.5416\n","Persisted the best model.\n","Epoch: 1/3... Step: 600... Loss: 1.9519... Val Loss: 2.2221 Perplexity: 9.2270\n","Persisted the best model.\n","Epoch: 1/3... Step: 650... Loss: 1.9391... Val Loss: 2.1936 Perplexity: 8.9673\n","Persisted the best model.\n","Epoch: 1/3... Step: 700... Loss: 1.9172... Val Loss: 2.1766 Perplexity: 8.8165\n","Persisted the best model.\n","Epoch: 1/3... Step: 750... Loss: 1.6581... Val Loss: 2.1210 Perplexity: 8.3398\n","Persisted the best model.\n","Epoch: 1/3... Step: 800... Loss: 1.6441... Val Loss: 2.1058 Perplexity: 8.2141\n","Persisted the best model.\n","Epoch: 1/3... Step: 850... Loss: 1.7086... Val Loss: 2.0686 Perplexity: 7.9137\n","Persisted the best model.\n","Epoch: 1/3... Step: 900... Loss: 1.6574... Val Loss: 2.0463 Perplexity: 7.7392\n","Persisted the best model.\n","Epoch: 1/3... Step: 950... Loss: 1.6023... Val Loss: 2.0383 Perplexity: 7.6775\n","Persisted the best model.\n","Epoch: 1/3... Step: 1000... Loss: 1.6107... Val Loss: 2.0090 Perplexity: 7.4561\n","Persisted the best model.\n","Epoch: 1/3... Step: 1050... Loss: 1.6689... Val Loss: 1.9787 Perplexity: 7.2331\n","Persisted the best model.\n","Epoch: 1/3... Step: 1100... Loss: 1.5630... Val Loss: 1.9780 Perplexity: 7.2282\n","Persisted the best model.\n","Epoch: 1/3... Step: 1150... Loss: 1.4601... Val Loss: 1.9589 Perplexity: 7.0915\n","Persisted the best model.\n","Epoch: 1/3... Step: 1200... Loss: 1.5306... Val Loss: 1.9588 Perplexity: 7.0912\n","Persisted the best model.\n","Epoch: 1/3... Step: 1250... Loss: 1.4800... Val Loss: 1.9234 Perplexity: 6.8441\n","Persisted the best model.\n","Epoch: 1/3... Step: 1300... Loss: 1.4146... Val Loss: 1.9211 Perplexity: 6.8287\n","Epoch: 1/3... Step: 1350... Loss: 1.3020... Val Loss: 1.9227 Perplexity: 6.8394\n","Persisted the best model.\n","Epoch: 1/3... Step: 1400... Loss: 1.4702... Val Loss: 1.9200 Perplexity: 6.8212\n","Persisted the best model.\n","Epoch: 1/3... Step: 1450... Loss: 1.3642... Val Loss: 1.9137 Perplexity: 6.7784\n","Persisted the best model.\n","Epoch: 1/3... Step: 1500... Loss: 1.4608... Val Loss: 1.8900 Perplexity: 6.6191\n","Epoch: 1/3... Step: 1550... Loss: 1.3033... Val Loss: 1.9029 Perplexity: 6.7053\n","Persisted the best model.\n","Epoch: 1/3... Step: 1600... Loss: 1.2430... Val Loss: 1.8742 Perplexity: 6.5158\n","Epoch: 1/3... Step: 1650... Loss: 1.3329... Val Loss: 1.8874 Perplexity: 6.6022\n","Persisted the best model.\n","Epoch: 1/3... Step: 1700... Loss: 1.3185... Val Loss: 1.8651 Perplexity: 6.4566\n","Persisted the best model.\n","Epoch: 1/3... Step: 1750... Loss: 1.2475... Val Loss: 1.8635 Perplexity: 6.4464\n","Persisted the best model.\n","Epoch: 1/3... Step: 1800... Loss: 1.2781... Val Loss: 1.8422 Perplexity: 6.3106\n","Epoch: 1/3... Step: 1850... Loss: 1.2217... Val Loss: 1.8453 Perplexity: 6.3299\n","Persisted the best model.\n","Epoch: 1/3... Step: 1900... Loss: 1.2533... Val Loss: 1.8394 Perplexity: 6.2925\n","Epoch: 1/3... Step: 1950... Loss: 1.2096... Val Loss: 1.8554 Perplexity: 6.3945\n","Epoch: 1/3... Step: 2000... Loss: 1.2436... Val Loss: 1.8431 Perplexity: 6.3162\n","Epoch: 1/3... Step: 2050... Loss: 1.3251... Val Loss: 1.8428 Perplexity: 6.3143\n","Persisted the best model.\n","Epoch: 1/3... Step: 2100... Loss: 1.1578... Val Loss: 1.8267 Perplexity: 6.2132\n","Epoch: 1/3... Step: 2150... Loss: 1.1267... Val Loss: 1.8421 Perplexity: 6.3096\n","Epoch: 1/3... Step: 2200... Loss: 1.2034... Val Loss: 1.8360 Perplexity: 6.2716\n","Persisted the best model.\n","Epoch: 1/3... Step: 2250... Loss: 1.1434... Val Loss: 1.8213 Perplexity: 6.1802\n","Persisted the best model.\n","Epoch: 1/3... Step: 2300... Loss: 1.1583... Val Loss: 1.8127 Perplexity: 6.1272\n","Epoch: 1/3... Step: 2350... Loss: 1.0586... Val Loss: 1.8439 Perplexity: 6.3213\n","Epoch: 1/3... Step: 2400... Loss: 1.0937... Val Loss: 1.8411 Perplexity: 6.3033\n","Persisted the best model.\n","Epoch: 1/3... Step: 2450... Loss: 1.1151... Val Loss: 1.8096 Perplexity: 6.1081\n","Epoch: 1/3... Step: 2500... Loss: 1.1092... Val Loss: 1.8108 Perplexity: 6.1154\n","Epoch: 1/3... Step: 2550... Loss: 1.1163... Val Loss: 1.8460 Perplexity: 6.3342\n","Epoch: 1/3... Step: 2600... Loss: 1.0602... Val Loss: 1.8274 Perplexity: 6.2177\n","Persisted the best model.\n","Epoch: 1/3... Step: 2650... Loss: 1.1750... Val Loss: 1.7947 Perplexity: 6.0175\n","Persisted the best model.\n","Epoch: 1/3... Step: 2700... Loss: 1.1482... Val Loss: 1.7865 Perplexity: 5.9686\n","Persisted the best model.\n","Epoch: 1/3... Step: 2750... Loss: 1.2013... Val Loss: 1.7749 Perplexity: 5.8999\n","Epoch: 1/3... Step: 2800... Loss: 1.1320... Val Loss: 1.8164 Perplexity: 6.1497\n","Epoch: 1/3... Step: 2850... Loss: 1.1118... Val Loss: 1.7827 Perplexity: 5.9456\n","Epoch: 1/3... Step: 2900... Loss: 1.0266... Val Loss: 1.8267 Perplexity: 6.2136\n","Epoch: 1/3... Step: 2950... Loss: 1.0637... Val Loss: 1.7850 Perplexity: 5.9598\n","Persisted the best model.\n","Epoch: 1/3... Step: 3000... Loss: 1.0580... Val Loss: 1.7715 Perplexity: 5.8798\n","Persisted the best model.\n","Epoch: 1/3... Step: 3050... Loss: 1.1921... Val Loss: 1.7516 Perplexity: 5.7640\n","Epoch: 1/3... Step: 3100... Loss: 1.1188... Val Loss: 1.7670 Perplexity: 5.8535\n","Epoch: 1/3... Step: 3150... Loss: 1.1590... Val Loss: 1.7967 Perplexity: 6.0295\n","Epoch: 1/3... Step: 3200... Loss: 0.9942... Val Loss: 1.8040 Perplexity: 6.0739\n","Epoch: 1/3... Step: 3250... Loss: 1.0508... Val Loss: 1.7699 Perplexity: 5.8705\n","Epoch: 1/3... Step: 3300... Loss: 1.0811... Val Loss: 1.7588 Perplexity: 5.8052\n","Persisted the best model.\n","Epoch: 1/3... Step: 3350... Loss: 1.0531... Val Loss: 1.7491 Perplexity: 5.7495\n","Epoch: 1/3... Step: 3400... Loss: 1.0681... Val Loss: 1.7556 Perplexity: 5.7868\n","Epoch: 1/3... Step: 3450... Loss: 0.9224... Val Loss: 1.7635 Perplexity: 5.8331\n","Epoch: 1/3... Step: 3500... Loss: 0.9528... Val Loss: 1.7688 Perplexity: 5.8636\n","Persisted the best model.\n","Epoch: 1/3... Step: 3550... Loss: 0.9862... Val Loss: 1.7477 Perplexity: 5.7413\n","Epoch: 1/3... Step: 3600... Loss: 1.0260... Val Loss: 1.7659 Perplexity: 5.8467\n","Epoch: 1/3... Step: 3650... Loss: 1.0165... Val Loss: 1.7573 Perplexity: 5.7966\n","Epoch: 1/3... Step: 3700... Loss: 0.9160... Val Loss: 1.7710 Perplexity: 5.8766\n","Epoch: 1/3... Step: 3750... Loss: 1.0533... Val Loss: 1.7614 Perplexity: 5.8203\n","Epoch: 1/3... Step: 3800... Loss: 0.9817... Val Loss: 1.7811 Perplexity: 5.9366\n","Epoch: 1/3... Step: 3850... Loss: 0.8786... Val Loss: 1.7656 Perplexity: 5.8450\n","Epoch: 1/3... Step: 3900... Loss: 0.9412... Val Loss: 1.7701 Perplexity: 5.8714\n","Epoch: 1/3... Step: 3950... Loss: 0.9045... Val Loss: 1.7888 Perplexity: 5.9822\n","Epoch: 1/3... Step: 4000... Loss: 1.0206... Val Loss: 1.7655 Perplexity: 5.8444\n","Epoch: 1/3... Step: 4050... Loss: 0.9502... Val Loss: 1.7979 Perplexity: 6.0370\n","Epoch: 1/3... Step: 4100... Loss: 0.9913... Val Loss: 1.8058 Perplexity: 6.0848\n","Epoch: 1/3... Step: 4150... Loss: 0.9597... Val Loss: 1.8085 Perplexity: 6.1014\n","Epoch: 1/3... Step: 4200... Loss: 0.9601... Val Loss: 1.7829 Perplexity: 5.9468\n","Epoch: 1/3... Step: 4250... Loss: 0.8145... Val Loss: 1.7837 Perplexity: 5.9521\n","Epoch: 1/3... Step: 4300... Loss: 0.8462... Val Loss: 1.7878 Perplexity: 5.9763\n","Epoch: 1/3... Step: 4350... Loss: 0.8345... Val Loss: 1.7918 Perplexity: 6.0000\n","Epoch: 1/3... Step: 4400... Loss: 0.9236... Val Loss: 1.7733 Perplexity: 5.8900\n","Epoch: 1/3... Step: 4450... Loss: 0.9685... Val Loss: 1.7799 Perplexity: 5.9293\n","Epoch: 1/3... Step: 4500... Loss: 0.8451... Val Loss: 1.7959 Perplexity: 6.0246\n","Epoch: 1/3... Step: 4550... Loss: 0.8076... Val Loss: 1.7879 Perplexity: 5.9770\n","Epoch: 1/3... Step: 4600... Loss: 0.8498... Val Loss: 1.8242 Perplexity: 6.1981\n","Epoch: 1/3... Step: 4650... Loss: 0.7752... Val Loss: 1.8225 Perplexity: 6.1873\n","Epoch: 1/3... Step: 4700... Loss: 0.8796... Val Loss: 1.7837 Perplexity: 5.9517\n","Epoch: 1/3... Step: 4750... Loss: 0.9665... Val Loss: 1.7814 Perplexity: 5.9382\n","Epoch: 1/3... Step: 4800... Loss: 0.9139... Val Loss: 1.7505 Perplexity: 5.7573\n","Epoch: 1/3... Step: 4850... Loss: 1.0404... Val Loss: 1.7484 Perplexity: 5.7454\n","Persisted the best model.\n","Epoch: 1/3... Step: 4900... Loss: 1.0361... Val Loss: 1.7277 Perplexity: 5.6275\n","Epoch: 1/3... Step: 4950... Loss: 0.9244... Val Loss: 1.7333 Perplexity: 5.6593\n","Epoch: 1/3... Step: 5000... Loss: 0.9383... Val Loss: 1.7385 Perplexity: 5.6891\n","Epoch: 1/3... Step: 5050... Loss: 0.7549... Val Loss: 1.7518 Perplexity: 5.7647\n","Epoch: 1/3... Step: 5100... Loss: 0.8810... Val Loss: 1.7534 Perplexity: 5.7745\n","Epoch: 1/3... Step: 5150... Loss: 0.8376... Val Loss: 1.7560 Perplexity: 5.7893\n","Persisted the best model.\n","Epoch: 1/3... Step: 5200... Loss: 0.8814... Val Loss: 1.7193 Perplexity: 5.5807\n","Epoch: 1/3... Step: 5250... Loss: 0.9477... Val Loss: 1.7251 Perplexity: 5.6130\n","Epoch: 1/3... Step: 5300... Loss: 0.8586... Val Loss: 1.7327 Perplexity: 5.6560\n","Epoch: 1/3... Step: 5350... Loss: 0.7982... Val Loss: 1.7275 Perplexity: 5.6266\n","Epoch: 1/3... Step: 5400... Loss: 0.8601... Val Loss: 1.7384 Perplexity: 5.6883\n","Epoch: 1/3... Step: 5450... Loss: 0.8355... Val Loss: 1.7376 Perplexity: 5.6838\n","Epoch: 1/3... Step: 5500... Loss: 0.8292... Val Loss: 1.7273 Perplexity: 5.6253\n","Persisted the best model.\n","Epoch: 1/3... Step: 5550... Loss: 0.9276... Val Loss: 1.6961 Perplexity: 5.4526\n","Epoch: 1/3... Step: 5600... Loss: 0.8038... Val Loss: 1.7558 Perplexity: 5.7881\n","Epoch: 1/3... Step: 5650... Loss: 0.8651... Val Loss: 1.7239 Perplexity: 5.6063\n","Epoch: 1/3... Step: 5700... Loss: 0.8951... Val Loss: 1.7289 Perplexity: 5.6344\n","Persisted the best model.\n","Epoch: 1/3... Step: 5750... Loss: 0.9576... Val Loss: 1.6928 Perplexity: 5.4347\n","Epoch: 1/3... Step: 5800... Loss: 0.8144... Val Loss: 1.7025 Perplexity: 5.4876\n","Epoch: 1/3... Step: 5850... Loss: 0.8423... Val Loss: 1.7050 Perplexity: 5.5014\n","Epoch: 1/3... Step: 5900... Loss: 0.8172... Val Loss: 1.7032 Perplexity: 5.4917\n","Persisted the best model.\n","Epoch: 1/3... Step: 5950... Loss: 0.8844... Val Loss: 1.6902 Perplexity: 5.4203\n","Persisted the best model.\n","Epoch: 1/3... Step: 6000... Loss: 0.9151... Val Loss: 1.6836 Perplexity: 5.3849\n","Persisted the best model.\n","Epoch: 1/3... Step: 6050... Loss: 0.8459... Val Loss: 1.6836 Perplexity: 5.3848\n","Persisted the best model.\n","Epoch: 1/3... Step: 6100... Loss: 0.8114... Val Loss: 1.6743 Perplexity: 5.3353\n","Epoch: 1/3... Step: 6150... Loss: 0.8810... Val Loss: 1.7180 Perplexity: 5.5733\n","Epoch: 1/3... Step: 6200... Loss: 0.8911... Val Loss: 1.7072 Perplexity: 5.5133\n","Epoch: 2/3... Step: 6250... Loss: 1.0205... Val Loss: 1.6894 Perplexity: 5.4163\n","Persisted the best model.\n","Epoch: 2/3... Step: 6300... Loss: 0.9159... Val Loss: 1.6622 Perplexity: 5.2708\n","Persisted the best model.\n","Epoch: 2/3... Step: 6350... Loss: 0.9823... Val Loss: 1.6617 Perplexity: 5.2682\n","Persisted the best model.\n","Epoch: 2/3... Step: 6400... Loss: 1.0817... Val Loss: 1.6557 Perplexity: 5.2368\n","Epoch: 2/3... Step: 6450... Loss: 0.9597... Val Loss: 1.6584 Perplexity: 5.2510\n","Persisted the best model.\n","Epoch: 2/3... Step: 6500... Loss: 0.9390... Val Loss: 1.6399 Perplexity: 5.1547\n","Persisted the best model.\n","Epoch: 2/3... Step: 6550... Loss: 1.0043... Val Loss: 1.6393 Perplexity: 5.1514\n","Epoch: 2/3... Step: 6600... Loss: 1.0383... Val Loss: 1.6432 Perplexity: 5.1716\n","Epoch: 2/3... Step: 6650... Loss: 0.9515... Val Loss: 1.6583 Perplexity: 5.2504\n","Epoch: 2/3... Step: 6700... Loss: 1.0782... Val Loss: 1.6606 Perplexity: 5.2625\n","Epoch: 2/3... Step: 6750... Loss: 1.0556... Val Loss: 1.6464 Perplexity: 5.1884\n","Persisted the best model.\n","Epoch: 2/3... Step: 6800... Loss: 0.9337... Val Loss: 1.6324 Perplexity: 5.1159\n","Persisted the best model.\n","Epoch: 2/3... Step: 6850... Loss: 0.9782... Val Loss: 1.6260 Perplexity: 5.0836\n","Epoch: 2/3... Step: 6900... Loss: 0.9584... Val Loss: 1.6288 Perplexity: 5.0980\n","Epoch: 2/3... Step: 6950... Loss: 0.8534... Val Loss: 1.6281 Perplexity: 5.0943\n","Epoch: 2/3... Step: 7000... Loss: 0.9187... Val Loss: 1.6618 Perplexity: 5.2686\n","Persisted the best model.\n","Epoch: 2/3... Step: 7050... Loss: 0.9139... Val Loss: 1.6162 Perplexity: 5.0338\n","Epoch: 2/3... Step: 7100... Loss: 0.8984... Val Loss: 1.6208 Perplexity: 5.0570\n","Epoch: 2/3... Step: 7150... Loss: 0.8446... Val Loss: 1.6283 Perplexity: 5.0953\n","Persisted the best model.\n","Epoch: 2/3... Step: 7200... Loss: 0.9438... Val Loss: 1.6108 Perplexity: 5.0070\n","Epoch: 2/3... Step: 7250... Loss: 0.9129... Val Loss: 1.6161 Perplexity: 5.0333\n","Epoch: 2/3... Step: 7300... Loss: 0.8405... Val Loss: 1.6260 Perplexity: 5.0837\n","Persisted the best model.\n","Epoch: 2/3... Step: 7350... Loss: 0.8904... Val Loss: 1.6047 Perplexity: 4.9763\n","Epoch: 2/3... Step: 7400... Loss: 0.9415... Val Loss: 1.6072 Perplexity: 4.9886\n","Epoch: 2/3... Step: 7450... Loss: 0.8296... Val Loss: 1.6092 Perplexity: 4.9986\n","Epoch: 2/3... Step: 7500... Loss: 0.7888... Val Loss: 1.6179 Perplexity: 5.0427\n","Epoch: 2/3... Step: 7550... Loss: 0.7109... Val Loss: 1.6476 Perplexity: 5.1944\n","Epoch: 2/3... Step: 7600... Loss: 0.7996... Val Loss: 1.6277 Perplexity: 5.0919\n","Epoch: 2/3... Step: 7650... Loss: 0.7851... Val Loss: 1.6408 Perplexity: 5.1591\n","Epoch: 2/3... Step: 7700... Loss: 0.7272... Val Loss: 1.6555 Perplexity: 5.2357\n","Epoch: 2/3... Step: 7750... Loss: 0.8313... Val Loss: 1.6174 Perplexity: 5.0401\n","Epoch: 2/3... Step: 7800... Loss: 0.7152... Val Loss: 1.6427 Perplexity: 5.1689\n","Epoch: 2/3... Step: 7850... Loss: 0.7538... Val Loss: 1.6243 Perplexity: 5.0748\n","Persisted the best model.\n","Epoch: 2/3... Step: 7900... Loss: 0.8464... Val Loss: 1.6009 Perplexity: 4.9575\n","Epoch: 2/3... Step: 7950... Loss: 0.7398... Val Loss: 1.6018 Perplexity: 4.9619\n","Epoch: 2/3... Step: 8000... Loss: 0.6984... Val Loss: 1.6099 Perplexity: 5.0023\n","Persisted the best model.\n","Epoch: 2/3... Step: 8050... Loss: 0.7830... Val Loss: 1.5937 Perplexity: 4.9221\n","Persisted the best model.\n","Epoch: 2/3... Step: 8100... Loss: 0.8435... Val Loss: 1.5912 Perplexity: 4.9095\n","Epoch: 2/3... Step: 8150... Loss: 0.8221... Val Loss: 1.6156 Perplexity: 5.0308\n","Epoch: 2/3... Step: 8200... Loss: 0.7046... Val Loss: 1.6344 Perplexity: 5.1265\n","Epoch: 2/3... Step: 8250... Loss: 0.7669... Val Loss: 1.6152 Perplexity: 5.0287\n","Epoch: 2/3... Step: 8300... Loss: 0.7640... Val Loss: 1.6279 Perplexity: 5.0934\n","Epoch: 2/3... Step: 8350... Loss: 0.7747... Val Loss: 1.6086 Perplexity: 4.9958\n","Epoch: 2/3... Step: 8400... Loss: 0.8091... Val Loss: 1.6179 Perplexity: 5.0427\n","Epoch: 2/3... Step: 8450... Loss: 0.7834... Val Loss: 1.6256 Perplexity: 5.0815\n","Epoch: 2/3... Step: 8500... Loss: 0.8004... Val Loss: 1.6235 Perplexity: 5.0707\n","Epoch: 2/3... Step: 8550... Loss: 0.7705... Val Loss: 1.6231 Perplexity: 5.0688\n","Epoch: 2/3... Step: 8600... Loss: 0.6668... Val Loss: 1.6499 Perplexity: 5.2065\n","Epoch: 2/3... Step: 8650... Loss: 0.8004... Val Loss: 1.6081 Perplexity: 4.9934\n","Epoch: 2/3... Step: 8700... Loss: 0.7717... Val Loss: 1.6264 Perplexity: 5.0854\n","Epoch: 2/3... Step: 8750... Loss: 0.7669... Val Loss: 1.6491 Perplexity: 5.2022\n","Epoch: 2/3... Step: 8800... Loss: 0.7791... Val Loss: 1.6366 Perplexity: 5.1377\n","Epoch: 2/3... Step: 8850... Loss: 0.8537... Val Loss: 1.6080 Perplexity: 4.9928\n","Epoch: 2/3... Step: 8900... Loss: 0.8407... Val Loss: 1.6157 Perplexity: 5.0312\n","Epoch: 2/3... Step: 8950... Loss: 0.7838... Val Loss: 1.5986 Perplexity: 4.9461\n","Epoch: 2/3... Step: 9000... Loss: 0.7338... Val Loss: 1.6045 Perplexity: 4.9755\n","Epoch: 2/3... Step: 9050... Loss: 0.7630... Val Loss: 1.5958 Perplexity: 4.9323\n","Epoch: 2/3... Step: 9100... Loss: 0.7084... Val Loss: 1.6181 Perplexity: 5.0436\n","Epoch: 2/3... Step: 9150... Loss: 0.8035... Val Loss: 1.6173 Perplexity: 5.0394\n","Epoch: 2/3... Step: 9200... Loss: 0.7398... Val Loss: 1.5945 Perplexity: 4.9259\n","Epoch: 2/3... Step: 9250... Loss: 0.8849... Val Loss: 1.6332 Perplexity: 5.1205\n","Epoch: 2/3... Step: 9300... Loss: 0.7487... Val Loss: 1.6474 Perplexity: 5.1937\n","Epoch: 2/3... Step: 9350... Loss: 0.6605... Val Loss: 1.6424 Perplexity: 5.1675\n","Epoch: 2/3... Step: 9400... Loss: 0.7291... Val Loss: 1.6408 Perplexity: 5.1592\n","Epoch: 2/3... Step: 9450... Loss: 0.7310... Val Loss: 1.6371 Perplexity: 5.1403\n","Epoch: 2/3... Step: 9500... Loss: 0.6840... Val Loss: 1.6256 Perplexity: 5.0817\n","Epoch: 2/3... Step: 9550... Loss: 0.7026... Val Loss: 1.6125 Perplexity: 5.0156\n","Epoch: 2/3... Step: 9600... Loss: 0.7392... Val Loss: 1.6225 Perplexity: 5.0657\n","Epoch: 2/3... Step: 9650... Loss: 0.6627... Val Loss: 1.6346 Perplexity: 5.1272\n","Epoch: 2/3... Step: 9700... Loss: 0.6883... Val Loss: 1.6311 Perplexity: 5.1094\n","Epoch: 2/3... Step: 9750... Loss: 0.6981... Val Loss: 1.6100 Perplexity: 5.0029\n","Epoch: 2/3... Step: 9800... Loss: 0.7209... Val Loss: 1.6225 Perplexity: 5.0660\n","Epoch: 2/3... Step: 9850... Loss: 0.6952... Val Loss: 1.6266 Perplexity: 5.0868\n","Epoch: 2/3... Step: 9900... Loss: 0.6342... Val Loss: 1.6525 Perplexity: 5.2198\n","Epoch: 2/3... Step: 9950... Loss: 0.7162... Val Loss: 1.6396 Perplexity: 5.1532\n","Epoch: 2/3... Step: 10000... Loss: 0.6764... Val Loss: 1.6392 Perplexity: 5.1510\n","Epoch: 2/3... Step: 10050... Loss: 0.7144... Val Loss: 1.6471 Perplexity: 5.1920\n","Epoch: 2/3... Step: 10100... Loss: 0.6706... Val Loss: 1.6450 Perplexity: 5.1810\n","Epoch: 2/3... Step: 10150... Loss: 0.5582... Val Loss: 1.6722 Perplexity: 5.3239\n","Epoch: 2/3... Step: 10200... Loss: 0.6963... Val Loss: 1.6351 Perplexity: 5.1301\n","Epoch: 2/3... Step: 10250... Loss: 0.6049... Val Loss: 1.6546 Perplexity: 5.2311\n","Epoch: 2/3... Step: 10300... Loss: 0.6805... Val Loss: 1.6739 Perplexity: 5.3329\n","Epoch: 2/3... Step: 10350... Loss: 0.6336... Val Loss: 1.6568 Perplexity: 5.2426\n","Epoch: 2/3... Step: 10400... Loss: 0.7031... Val Loss: 1.6340 Perplexity: 5.1241\n","Epoch: 2/3... Step: 10450... Loss: 0.6435... Val Loss: 1.6839 Perplexity: 5.3863\n","Epoch: 2/3... Step: 10500... Loss: 0.6000... Val Loss: 1.6826 Perplexity: 5.3796\n","Epoch: 2/3... Step: 10550... Loss: 0.6303... Val Loss: 1.6830 Perplexity: 5.3817\n","Epoch: 2/3... Step: 10600... Loss: 0.6689... Val Loss: 1.6609 Perplexity: 5.2641\n","Epoch: 2/3... Step: 10650... Loss: 0.6615... Val Loss: 1.6806 Perplexity: 5.3688\n","Epoch: 2/3... Step: 10700... Loss: 0.6058... Val Loss: 1.6759 Perplexity: 5.3434\n","Epoch: 2/3... Step: 10750... Loss: 0.5716... Val Loss: 1.7177 Perplexity: 5.5714\n","Epoch: 2/3... Step: 10800... Loss: 0.5898... Val Loss: 1.7580 Perplexity: 5.8011\n","Epoch: 2/3... Step: 10850... Loss: 0.5504... Val Loss: 1.7058 Perplexity: 5.5058\n","Epoch: 2/3... Step: 10900... Loss: 0.6307... Val Loss: 1.6920 Perplexity: 5.4305\n","Epoch: 2/3... Step: 10950... Loss: 0.6623... Val Loss: 1.6980 Perplexity: 5.4628\n","Epoch: 2/3... Step: 11000... Loss: 0.7121... Val Loss: 1.6451 Perplexity: 5.1817\n","Epoch: 2/3... Step: 11050... Loss: 0.7139... Val Loss: 1.6415 Perplexity: 5.1631\n","Epoch: 2/3... Step: 11100... Loss: 0.7571... Val Loss: 1.6620 Perplexity: 5.2696\n","Epoch: 2/3... Step: 11150... Loss: 0.6876... Val Loss: 1.6447 Perplexity: 5.1794\n","Epoch: 2/3... Step: 11200... Loss: 0.6256... Val Loss: 1.6546 Perplexity: 5.2312\n","Epoch: 2/3... Step: 11250... Loss: 0.5821... Val Loss: 1.6518 Perplexity: 5.2162\n","Epoch: 2/3... Step: 11300... Loss: 0.5715... Val Loss: 1.6539 Perplexity: 5.2273\n","Epoch: 2/3... Step: 11350... Loss: 0.6868... Val Loss: 1.6556 Perplexity: 5.2360\n","Epoch: 2/3... Step: 11400... Loss: 0.6770... Val Loss: 1.6415 Perplexity: 5.1628\n","Epoch: 2/3... Step: 11450... Loss: 0.6526... Val Loss: 1.6519 Perplexity: 5.2171\n","Epoch: 2/3... Step: 11500... Loss: 0.5888... Val Loss: 1.6444 Perplexity: 5.1779\n","Epoch: 2/3... Step: 11550... Loss: 0.5449... Val Loss: 1.6827 Perplexity: 5.3803\n","Epoch: 2/3... Step: 11600... Loss: 0.6907... Val Loss: 1.6602 Perplexity: 5.2603\n","Epoch: 2/3... Step: 11650... Loss: 0.6041... Val Loss: 1.6562 Perplexity: 5.2393\n","Epoch: 2/3... Step: 11700... Loss: 0.6053... Val Loss: 1.6629 Perplexity: 5.2747\n","Epoch: 2/3... Step: 11750... Loss: 0.6758... Val Loss: 1.6430 Perplexity: 5.1708\n","Epoch: 2/3... Step: 11800... Loss: 0.6217... Val Loss: 1.6267 Perplexity: 5.0870\n","Epoch: 2/3... Step: 11850... Loss: 0.5947... Val Loss: 1.6398 Perplexity: 5.1542\n","Epoch: 2/3... Step: 11900... Loss: 0.6301... Val Loss: 1.6431 Perplexity: 5.1713\n","Epoch: 2/3... Step: 11950... Loss: 0.7269... Val Loss: 1.6378 Perplexity: 5.1436\n","Epoch: 2/3... Step: 12000... Loss: 0.6031... Val Loss: 1.6458 Perplexity: 5.1851\n","Epoch: 2/3... Step: 12050... Loss: 0.6831... Val Loss: 1.6095 Perplexity: 5.0005\n","Epoch: 2/3... Step: 12100... Loss: 0.7485... Val Loss: 1.6352 Perplexity: 5.1303\n","Epoch: 2/3... Step: 12150... Loss: 0.6845... Val Loss: 1.6310 Perplexity: 5.1091\n","Epoch: 2/3... Step: 12200... Loss: 0.6772... Val Loss: 1.6222 Perplexity: 5.0642\n","Epoch: 2/3... Step: 12250... Loss: 0.6615... Val Loss: 1.6054 Perplexity: 4.9799\n","Epoch: 2/3... Step: 12300... Loss: 0.5807... Val Loss: 1.6163 Perplexity: 5.0345\n","Epoch: 2/3... Step: 12350... Loss: 0.6439... Val Loss: 1.6157 Perplexity: 5.0316\n","Epoch: 2/3... Step: 12400... Loss: 0.6839... Val Loss: 1.6108 Perplexity: 5.0069\n","Epoch: 3/3... Step: 12450... Loss: 0.7558... Val Loss: 1.6146 Perplexity: 5.0260\n","Epoch: 3/3... Step: 12500... Loss: 0.7721... Val Loss: 1.6074 Perplexity: 4.9897\n","Epoch: 3/3... Step: 12550... Loss: 0.8892... Val Loss: 1.6177 Perplexity: 5.0415\n","Epoch: 3/3... Step: 12600... Loss: 0.8112... Val Loss: 1.5933 Perplexity: 4.9198\n","Epoch: 3/3... Step: 12650... Loss: 0.7585... Val Loss: 1.6080 Perplexity: 4.9929\n","Persisted the best model.\n","Epoch: 3/3... Step: 12700... Loss: 0.8757... Val Loss: 1.5851 Perplexity: 4.8797\n","Epoch: 3/3... Step: 12750... Loss: 0.8444... Val Loss: 1.6301 Perplexity: 5.1045\n","Epoch: 3/3... Step: 12800... Loss: 0.7939... Val Loss: 1.6121 Perplexity: 5.0135\n","Epoch: 3/3... Step: 12850... Loss: 0.7756... Val Loss: 1.6180 Perplexity: 5.0431\n","Epoch: 3/3... Step: 12900... Loss: 0.8883... Val Loss: 1.6283 Perplexity: 5.0951\n","Epoch: 3/3... Step: 12950... Loss: 0.8159... Val Loss: 1.6004 Perplexity: 4.9552\n","Epoch: 3/3... Step: 13000... Loss: 0.7770... Val Loss: 1.5951 Perplexity: 4.9289\n","Epoch: 3/3... Step: 13050... Loss: 0.7671... Val Loss: 1.5947 Perplexity: 4.9270\n","Epoch: 3/3... Step: 13100... Loss: 0.8140... Val Loss: 1.6152 Perplexity: 5.0289\n","Epoch: 3/3... Step: 13150... Loss: 0.8506... Val Loss: 1.5935 Perplexity: 4.9208\n","Epoch: 3/3... Step: 13200... Loss: 0.7359... Val Loss: 1.6108 Perplexity: 5.0067\n","Epoch: 3/3... Step: 13250... Loss: 0.7883... Val Loss: 1.6113 Perplexity: 5.0094\n","Epoch: 3/3... Step: 13300... Loss: 0.7213... Val Loss: 1.6317 Perplexity: 5.1123\n","Epoch: 3/3... Step: 13350... Loss: 0.6649... Val Loss: 1.5985 Perplexity: 4.9458\n","Persisted the best model.\n","Epoch: 3/3... Step: 13400... Loss: 0.6559... Val Loss: 1.5714 Perplexity: 4.8134\n","Persisted the best model.\n","Epoch: 3/3... Step: 13450... Loss: 0.8041... Val Loss: 1.5669 Perplexity: 4.7918\n","Epoch: 3/3... Step: 13500... Loss: 0.6305... Val Loss: 1.5918 Perplexity: 4.9127\n","Epoch: 3/3... Step: 13550... Loss: 0.7036... Val Loss: 1.6007 Perplexity: 4.9563\n","Epoch: 3/3... Step: 13600... Loss: 0.7349... Val Loss: 1.6117 Perplexity: 5.0112\n","Epoch: 3/3... Step: 13650... Loss: 0.6647... Val Loss: 1.6257 Perplexity: 5.0820\n","Epoch: 3/3... Step: 13700... Loss: 0.5830... Val Loss: 1.6057 Perplexity: 4.9815\n","Epoch: 3/3... Step: 13750... Loss: 0.6109... Val Loss: 1.6169 Perplexity: 5.0372\n","Epoch: 3/3... Step: 13800... Loss: 0.5962... Val Loss: 1.6380 Perplexity: 5.1449\n","Epoch: 3/3... Step: 13850... Loss: 0.6390... Val Loss: 1.6164 Perplexity: 5.0349\n","Epoch: 3/3... Step: 13900... Loss: 0.7004... Val Loss: 1.6239 Perplexity: 5.0729\n","Epoch: 3/3... Step: 13950... Loss: 0.6774... Val Loss: 1.5993 Perplexity: 4.9496\n","Epoch: 3/3... Step: 14000... Loss: 0.5763... Val Loss: 1.6218 Perplexity: 5.0621\n","Epoch: 3/3... Step: 14050... Loss: 0.6304... Val Loss: 1.6058 Perplexity: 4.9821\n","Epoch: 3/3... Step: 14100... Loss: 0.6338... Val Loss: 1.5904 Perplexity: 4.9059\n","Epoch: 3/3... Step: 14150... Loss: 0.6183... Val Loss: 1.5909 Perplexity: 4.9084\n","Epoch: 3/3... Step: 14200... Loss: 0.6112... Val Loss: 1.5919 Perplexity: 4.9130\n","Epoch: 3/3... Step: 14250... Loss: 0.6695... Val Loss: 1.5833 Perplexity: 4.8711\n","Epoch: 3/3... Step: 14300... Loss: 0.7357... Val Loss: 1.5860 Perplexity: 4.8843\n","Epoch: 3/3... Step: 14350... Loss: 0.6341... Val Loss: 1.6023 Perplexity: 4.9644\n","Epoch: 3/3... Step: 14400... Loss: 0.6068... Val Loss: 1.6159 Perplexity: 5.0327\n","Epoch: 3/3... Step: 14450... Loss: 0.6002... Val Loss: 1.6258 Perplexity: 5.0825\n","Epoch: 3/3... Step: 14500... Loss: 0.5965... Val Loss: 1.6044 Perplexity: 4.9747\n","Epoch: 3/3... Step: 14550... Loss: 0.5543... Val Loss: 1.5850 Perplexity: 4.8794\n","Epoch: 3/3... Step: 14600... Loss: 0.6239... Val Loss: 1.6067 Perplexity: 4.9866\n","Epoch: 3/3... Step: 14650... Loss: 0.6370... Val Loss: 1.6156 Perplexity: 5.0311\n","Epoch: 3/3... Step: 14700... Loss: 0.6360... Val Loss: 1.6091 Perplexity: 4.9981\n","Epoch: 3/3... Step: 14750... Loss: 0.6362... Val Loss: 1.6085 Perplexity: 4.9953\n","Epoch: 3/3... Step: 14800... Loss: 0.6229... Val Loss: 1.6298 Perplexity: 5.1028\n","Epoch: 3/3... Step: 14850... Loss: 0.6358... Val Loss: 1.6373 Perplexity: 5.1413\n","Epoch: 3/3... Step: 14900... Loss: 0.7186... Val Loss: 1.6105 Perplexity: 5.0053\n","Epoch: 3/3... Step: 14950... Loss: 0.5941... Val Loss: 1.6650 Perplexity: 5.2856\n","Epoch: 3/3... Step: 15000... Loss: 0.6580... Val Loss: 1.6294 Perplexity: 5.1009\n","Epoch: 3/3... Step: 15050... Loss: 0.7323... Val Loss: 1.6251 Perplexity: 5.0790\n","Epoch: 3/3... Step: 15100... Loss: 0.6798... Val Loss: 1.6048 Perplexity: 4.9768\n","Epoch: 3/3... Step: 15150... Loss: 0.6598... Val Loss: 1.5811 Perplexity: 4.8602\n","Epoch: 3/3... Step: 15200... Loss: 0.6310... Val Loss: 1.5995 Perplexity: 4.9508\n","Epoch: 3/3... Step: 15250... Loss: 0.5739... Val Loss: 1.6156 Perplexity: 5.0310\n","Epoch: 3/3... Step: 15300... Loss: 0.5686... Val Loss: 1.6368 Perplexity: 5.1387\n","Epoch: 3/3... Step: 15350... Loss: 0.7018... Val Loss: 1.6326 Perplexity: 5.1174\n","Epoch: 3/3... Step: 15400... Loss: 0.6846... Val Loss: 1.6135 Perplexity: 5.0204\n","Epoch: 3/3... Step: 15450... Loss: 0.6248... Val Loss: 1.6332 Perplexity: 5.1201\n","Epoch: 3/3... Step: 15500... Loss: 0.5851... Val Loss: 1.6315 Perplexity: 5.1116\n","Epoch: 3/3... Step: 15550... Loss: 0.6164... Val Loss: 1.6550 Perplexity: 5.2329\n","Epoch: 3/3... Step: 15600... Loss: 0.6242... Val Loss: 1.6364 Perplexity: 5.1369\n","Epoch: 3/3... Step: 15650... Loss: 0.5525... Val Loss: 1.6163 Perplexity: 5.0346\n","Epoch: 3/3... Step: 15700... Loss: 0.6026... Val Loss: 1.6107 Perplexity: 5.0064\n","Epoch: 3/3... Step: 15750... Loss: 0.5708... Val Loss: 1.6052 Perplexity: 4.9789\n","Epoch: 3/3... Step: 15800... Loss: 0.5459... Val Loss: 1.6176 Perplexity: 5.0408\n","Epoch: 3/3... Step: 15850... Loss: 0.5445... Val Loss: 1.6380 Perplexity: 5.1449\n","Epoch: 3/3... Step: 15900... Loss: 0.5464... Val Loss: 1.6211 Perplexity: 5.0586\n","Epoch: 3/3... Step: 15950... Loss: 0.6668... Val Loss: 1.6015 Perplexity: 4.9607\n","Epoch: 3/3... Step: 16000... Loss: 0.5875... Val Loss: 1.6262 Perplexity: 5.0845\n","Epoch: 3/3... Step: 16050... Loss: 0.5635... Val Loss: 1.6309 Perplexity: 5.1087\n","Epoch: 3/3... Step: 16100... Loss: 0.5422... Val Loss: 1.6128 Perplexity: 5.0167\n","Epoch: 3/3... Step: 16150... Loss: 0.6811... Val Loss: 1.6227 Perplexity: 5.0668\n","Epoch: 3/3... Step: 16200... Loss: 0.5540... Val Loss: 1.6387 Perplexity: 5.1485\n","Epoch: 3/3... Step: 16250... Loss: 0.5527... Val Loss: 1.6459 Perplexity: 5.1859\n","Epoch: 3/3... Step: 16300... Loss: 0.5793... Val Loss: 1.6547 Perplexity: 5.2315\n","Epoch: 3/3... Step: 16350... Loss: 0.5241... Val Loss: 1.6617 Perplexity: 5.2684\n","Epoch: 3/3... Step: 16400... Loss: 0.5307... Val Loss: 1.6269 Perplexity: 5.0883\n","Epoch: 3/3... Step: 16450... Loss: 0.5830... Val Loss: 1.6059 Perplexity: 4.9825\n","Epoch: 3/3... Step: 16500... Loss: 0.5132... Val Loss: 1.6351 Perplexity: 5.1300\n","Epoch: 3/3... Step: 16550... Loss: 0.6343... Val Loss: 1.6257 Perplexity: 5.0820\n","Epoch: 3/3... Step: 16600... Loss: 0.6327... Val Loss: 1.6188 Perplexity: 5.0470\n","Epoch: 3/3... Step: 16650... Loss: 0.4786... Val Loss: 1.6539 Perplexity: 5.2273\n","Epoch: 3/3... Step: 16700... Loss: 0.5150... Val Loss: 1.6568 Perplexity: 5.2426\n","Epoch: 3/3... Step: 16750... Loss: 0.5525... Val Loss: 1.6775 Perplexity: 5.3520\n","Epoch: 3/3... Step: 16800... Loss: 0.5440... Val Loss: 1.6593 Perplexity: 5.2555\n","Epoch: 3/3... Step: 16850... Loss: 0.5387... Val Loss: 1.6615 Perplexity: 5.2671\n","Epoch: 3/3... Step: 16900... Loss: 0.4979... Val Loss: 1.6696 Perplexity: 5.3100\n","Epoch: 3/3... Step: 16950... Loss: 0.5057... Val Loss: 1.6897 Perplexity: 5.4181\n","Epoch: 3/3... Step: 17000... Loss: 0.4394... Val Loss: 1.6895 Perplexity: 5.4166\n","Epoch: 3/3... Step: 17050... Loss: 0.4736... Val Loss: 1.7252 Perplexity: 5.6137\n","Epoch: 3/3... Step: 17100... Loss: 0.5295... Val Loss: 1.6738 Perplexity: 5.3323\n","Epoch: 3/3... Step: 17150... Loss: 0.6566... Val Loss: 1.6688 Perplexity: 5.3060\n","Epoch: 3/3... Step: 17200... Loss: 0.5758... Val Loss: 1.6480 Perplexity: 5.1964\n","Epoch: 3/3... Step: 17250... Loss: 0.6848... Val Loss: 1.6384 Perplexity: 5.1471\n","Epoch: 3/3... Step: 17300... Loss: 0.6132... Val Loss: 1.6501 Perplexity: 5.2073\n","Epoch: 3/3... Step: 17350... Loss: 0.5701... Val Loss: 1.6411 Perplexity: 5.1609\n","Epoch: 3/3... Step: 17400... Loss: 0.5146... Val Loss: 1.6501 Perplexity: 5.2076\n","Epoch: 3/3... Step: 17450... Loss: 0.5150... Val Loss: 1.6477 Perplexity: 5.1951\n","Epoch: 3/3... Step: 17500... Loss: 0.4890... Val Loss: 1.6326 Perplexity: 5.1170\n","Epoch: 3/3... Step: 17550... Loss: 0.5312... Val Loss: 1.6299 Perplexity: 5.1035\n","Epoch: 3/3... Step: 17600... Loss: 0.5818... Val Loss: 1.6147 Perplexity: 5.0264\n","Epoch: 3/3... Step: 17650... Loss: 0.5695... Val Loss: 1.6411 Perplexity: 5.1607\n","Epoch: 3/3... Step: 17700... Loss: 0.5640... Val Loss: 1.6458 Perplexity: 5.1854\n","Epoch: 3/3... Step: 17750... Loss: 0.4889... Val Loss: 1.6639 Perplexity: 5.2801\n","Epoch: 3/3... Step: 17800... Loss: 0.5810... Val Loss: 1.6568 Perplexity: 5.2425\n","Epoch: 3/3... Step: 17850... Loss: 0.5221... Val Loss: 1.6761 Perplexity: 5.3447\n","Epoch: 3/3... Step: 17900... Loss: 0.5221... Val Loss: 1.6496 Perplexity: 5.2051\n","Epoch: 3/3... Step: 17950... Loss: 0.5445... Val Loss: 1.6414 Perplexity: 5.1622\n","Epoch: 3/3... Step: 18000... Loss: 0.6355... Val Loss: 1.6536 Perplexity: 5.2257\n","Epoch: 3/3... Step: 18050... Loss: 0.6224... Val Loss: 1.6401 Perplexity: 5.1559\n","Epoch: 3/3... Step: 18100... Loss: 0.5419... Val Loss: 1.6354 Perplexity: 5.1313\n","Epoch: 3/3... Step: 18150... Loss: 0.5464... Val Loss: 1.6203 Perplexity: 5.0546\n","Epoch: 3/3... Step: 18200... Loss: 0.5671... Val Loss: 1.6351 Perplexity: 5.1297\n","Epoch: 3/3... Step: 18250... Loss: 0.6378... Val Loss: 1.5990 Perplexity: 4.9483\n","Epoch: 3/3... Step: 18300... Loss: 0.6235... Val Loss: 1.6156 Perplexity: 5.0311\n","Epoch: 3/3... Step: 18350... Loss: 0.6101... Val Loss: 1.6439 Perplexity: 5.1753\n","Epoch: 3/3... Step: 18400... Loss: 0.5720... Val Loss: 1.6146 Perplexity: 5.0259\n","Epoch: 3/3... Step: 18450... Loss: 0.6347... Val Loss: 1.6203 Perplexity: 5.0544\n","Epoch: 3/3... Step: 18500... Loss: 0.5330... Val Loss: 1.6359 Perplexity: 5.1343\n","Epoch: 3/3... Step: 18550... Loss: 0.6515... Val Loss: 1.6410 Perplexity: 5.1605\n","Epoch: 3/3... Step: 18600... Loss: 0.6638... Val Loss: 1.6264 Perplexity: 5.0854\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OhtcvzSt930G","colab_type":"code","colab":{}},"source":["\n","def save_model(net):\n","  model_name = 'drive/datasets/SCRNN.net'\n","\n","  checkpoint = {'n_hidden': net.n_hidden,\n","                'n_layers': net.n_layers,\n","                'state_dict': net.state_dict(),\n","                'tokens': dictionary,\n","                'int2token': int2token,\n","                'token2int': token2int}\n","\n","  with open(model_name, 'wb') as f:\n","      torch.save(checkpoint, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OmEJbYpKJK3","colab_type":"code","colab":{}},"source":["import torch\n","model_name = 'drive/datasets/SCRNN.net'\n","\n","checkpoint = {}\n","\n","with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUFQ8MLH930K","colab_type":"code","colab":{}},"source":["def predict(token2int, int2token, net, token, h=None, top_k=None):\n","        \n","        x = np.array([[token2int[token]]])\n","        \n","        inputs = torch.from_numpy(x)\n","\n","        # detach hidden state from history\n","        h = tuple([each.data for each in h])\n","\n","        out, h, out_features = net(inputs, h)\n","\n","        p = F.softmax(out, dim=1).data\n","\n","        p, top_tokens = p.topk(top_k)\n","        top_tokens = top_tokens.numpy().squeeze()\n","        \n","        p = p.numpy().squeeze()\n","        token = np.random.choice(top_tokens, p=p/p.sum())\n","        \n","        return int2token[token], h, out_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqpBuHQo930P","colab_type":"code","colab":{}},"source":["def sample(token2int, int2token, net, size, prime=[], top_k=5):\n","        \n","    net.cpu()\n","    net.eval() # eval mode\n","    \n","    h = net.init_hidden(1)\n","    \n","    features = []\n","    tokens = prime\n","    for token in prime:\n","        token, h, out_features = predict(token2int, int2token, net, token, h, top_k=top_k)\n","        features.append(out_features)\n","\n","    if size > 0:\n","      tokens.append(token)\n","\n","      for ii in range(size):\n","          token, h, out_features = predict(token2int, int2token, net, tokens[-1], h, top_k=top_k)\n","          tokens.append(token)\n","          features.append(out_features)\n","\n","    return ''.join(tokens), features"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qzGgmBz_930V","colab_type":"text"},"source":["## Loading a checkpoint"]},{"cell_type":"code","metadata":{"id":"I9MmgC2R930V","colab_type":"code","colab":{}},"source":["with open('drive/datasets/SCRNN.net', 'rb') as f:\n","    checkpoint = torch.load(f)\n","    \n","loaded = SCRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n","loaded.load_state_dict(checkpoint['state_dict'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHpyotWA930Z","colab_type":"code","outputId":"1fdf5798-4632-4eec-e3d1-584200dd5426","executionInfo":{"status":"ok","timestamp":1556352206320,"user_tz":-180,"elapsed":984,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_on_gpu = False\n","\n","code, features = sample(checkpoint['token2int'], checkpoint['int2token'], loaded, 0, top_k=5, prime=['while', '('])\n","\n","print(features[0].shape)\n","print(features[1].shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([1, 512])\n","torch.Size([1, 512])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jaLRL77W930b","colab_type":"code","outputId":"6259ebd4-9e0d-4a1b-ba3a-6aeb471600e4","executionInfo":{"status":"ok","timestamp":1556013512881,"user_tz":-180,"elapsed":604,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["''.join([int2token[x] for x in val_methods[510001:510100]])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'next(t2-t1);<unk>();<unk>();break;case<unk>next(t2-t1);<unk>();<unk>();boolean<unk>=true;for(inti=num;<unk>i<<unk>;i){<unk>=(<unk>[i]num)(<unk>[i]num);}if(<unk>){state=<unk>;if(!<unk>[num]){<unk>();}}if(<unk>'"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"id":"J8xhcHEalkr8","colab_type":"code","outputId":"3c46b720-731f-4077-a034-f56f5b28d13d","executionInfo":{"status":"ok","timestamp":1556013651518,"user_tz":-180,"elapsed":895,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["[int2token[x] for x in val_methods[510001:510010]]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['next', '(', 't2', '-', 't1', ')', ';', '<unk>', '(']"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"markdown","metadata":{"id":"ACZF9ktyFTWA","colab_type":"text"},"source":["**Feature extraction**"]},{"cell_type":"code","metadata":{"id":"khKMcqxxFZFq","colab_type":"code","outputId":"41250f05-8c16-4d6b-b917-1ad5e5575c9a","executionInfo":{"status":"ok","timestamp":1556389327412,"user_tz":-180,"elapsed":1299900,"user":{"displayName":"Dariuš Butkevičius","photoUrl":"","userId":"02979305196329057997"}},"colab":{"base_uri":"https://localhost:8080/","height":4753}},"source":["path = 'drive/datasets/smells-dataset'\n","target = 'drive/datasets/features_lstm.csv'\n","unknown_token = '<unk>'\n","features_num = 512\n","print_every = 5\n","\n","with open('drive/datasets/SCRNN.net', 'rb') as f:\n","    checkpoint = torch.load(f)\n","    \n","loaded = SCRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n","loaded.load_state_dict(checkpoint['state_dict'])\n","\n","i = 0\n","_, dirnames, _ = next(os.walk(path))\n","\n","output_dataset = pd.DataFrame(index=range(len(dirnames)), columns=list(range(features_num)) + ['sevrity', 'smell', 'id'])\n","for dir in dirnames:\n","  parts = dir.split('_')\n","  smell = parts[0]\n","  severity = parts[1]\n","  id = parts[2]\n","  \n","  subpath = os.path.join(path, dir)\n","  _, _, filenames = next(os.walk(subpath))\n","  \n","  methods_features = np.zeros((1,features_num))\n","  for file in filenames:\n","    file_path = os.path.join(subpath, file)\n","    with open(file_path, 'r') as f:\n","      content = f.read()\n","      content_tokens = content.split(',')\n","      content_tokens = [x.replace('\\n', '') if x.replace('\\n','') in checkpoint['tokens'] else unknown_token for x in content_tokens]\n","\n","      code, features = sample(checkpoint['token2int'], checkpoint['int2token'], loaded, 0, prime=content_tokens)\n","\n","      method_features = np.mean(list(map(lambda x: x.detach().numpy(), features)), axis = 0)\n","      methods_features += method_features\n","\n","  methods_features /= len(filenames)\n","  output_dataset.iloc[i] = np.concatenate((methods_features, np.array([severity, smell, id]).reshape(1,-1)), axis = 1)\n","\n","  i += 1\n","  if i % print_every == 0:\n","    print(f'Processed {i}/{len(dirnames)}')\n","\n","output = output_dataset.to_csv()\n","with open(target, \"w\") as f:\n","  f.write(output)\n","\n","        "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Processed 5/1379\n","Processed 10/1379\n","Processed 15/1379\n","Processed 20/1379\n","Processed 25/1379\n","Processed 30/1379\n","Processed 35/1379\n","Processed 40/1379\n","Processed 45/1379\n","Processed 50/1379\n","Processed 55/1379\n","Processed 60/1379\n","Processed 65/1379\n","Processed 70/1379\n","Processed 75/1379\n","Processed 80/1379\n","Processed 85/1379\n","Processed 90/1379\n","Processed 95/1379\n","Processed 100/1379\n","Processed 105/1379\n","Processed 110/1379\n","Processed 115/1379\n","Processed 120/1379\n","Processed 125/1379\n","Processed 130/1379\n","Processed 135/1379\n","Processed 140/1379\n","Processed 145/1379\n","Processed 150/1379\n","Processed 155/1379\n","Processed 160/1379\n","Processed 165/1379\n","Processed 170/1379\n","Processed 175/1379\n","Processed 180/1379\n","Processed 185/1379\n","Processed 190/1379\n","Processed 195/1379\n","Processed 200/1379\n","Processed 205/1379\n","Processed 210/1379\n","Processed 215/1379\n","Processed 220/1379\n","Processed 225/1379\n","Processed 230/1379\n","Processed 235/1379\n","Processed 240/1379\n","Processed 245/1379\n","Processed 250/1379\n","Processed 255/1379\n","Processed 260/1379\n","Processed 265/1379\n","Processed 270/1379\n","Processed 275/1379\n","Processed 280/1379\n","Processed 285/1379\n","Processed 290/1379\n","Processed 295/1379\n","Processed 300/1379\n","Processed 305/1379\n","Processed 310/1379\n","Processed 315/1379\n","Processed 320/1379\n","Processed 325/1379\n","Processed 330/1379\n","Processed 335/1379\n","Processed 340/1379\n","Processed 345/1379\n","Processed 350/1379\n","Processed 355/1379\n","Processed 360/1379\n","Processed 365/1379\n","Processed 370/1379\n","Processed 375/1379\n","Processed 380/1379\n","Processed 385/1379\n","Processed 390/1379\n","Processed 395/1379\n","Processed 400/1379\n","Processed 405/1379\n","Processed 410/1379\n","Processed 415/1379\n","Processed 420/1379\n","Processed 425/1379\n","Processed 430/1379\n","Processed 435/1379\n","Processed 440/1379\n","Processed 445/1379\n","Processed 450/1379\n","Processed 455/1379\n","Processed 460/1379\n","Processed 465/1379\n","Processed 470/1379\n","Processed 475/1379\n","Processed 480/1379\n","Processed 485/1379\n","Processed 490/1379\n","Processed 495/1379\n","Processed 500/1379\n","Processed 505/1379\n","Processed 510/1379\n","Processed 515/1379\n","Processed 520/1379\n","Processed 525/1379\n","Processed 530/1379\n","Processed 535/1379\n","Processed 540/1379\n","Processed 545/1379\n","Processed 550/1379\n","Processed 555/1379\n","Processed 560/1379\n","Processed 565/1379\n","Processed 570/1379\n","Processed 575/1379\n","Processed 580/1379\n","Processed 585/1379\n","Processed 590/1379\n","Processed 595/1379\n","Processed 600/1379\n","Processed 605/1379\n","Processed 610/1379\n","Processed 615/1379\n","Processed 620/1379\n","Processed 625/1379\n","Processed 630/1379\n","Processed 635/1379\n","Processed 640/1379\n","Processed 645/1379\n","Processed 650/1379\n","Processed 655/1379\n","Processed 660/1379\n","Processed 665/1379\n","Processed 670/1379\n","Processed 675/1379\n","Processed 680/1379\n","Processed 685/1379\n","Processed 690/1379\n","Processed 695/1379\n","Processed 700/1379\n","Processed 705/1379\n","Processed 710/1379\n","Processed 715/1379\n","Processed 720/1379\n","Processed 725/1379\n","Processed 730/1379\n","Processed 735/1379\n","Processed 740/1379\n","Processed 745/1379\n","Processed 750/1379\n","Processed 755/1379\n","Processed 760/1379\n","Processed 765/1379\n","Processed 770/1379\n","Processed 775/1379\n","Processed 780/1379\n","Processed 785/1379\n","Processed 790/1379\n","Processed 795/1379\n","Processed 800/1379\n","Processed 805/1379\n","Processed 810/1379\n","Processed 815/1379\n","Processed 820/1379\n","Processed 825/1379\n","Processed 830/1379\n","Processed 835/1379\n","Processed 840/1379\n","Processed 845/1379\n","Processed 850/1379\n","Processed 855/1379\n","Processed 860/1379\n","Processed 865/1379\n","Processed 870/1379\n","Processed 875/1379\n","Processed 880/1379\n","Processed 885/1379\n","Processed 890/1379\n","Processed 895/1379\n","Processed 900/1379\n","Processed 905/1379\n","Processed 910/1379\n","Processed 915/1379\n","Processed 920/1379\n","Processed 925/1379\n","Processed 930/1379\n","Processed 935/1379\n","Processed 940/1379\n","Processed 945/1379\n","Processed 950/1379\n","Processed 955/1379\n","Processed 960/1379\n","Processed 965/1379\n","Processed 970/1379\n","Processed 975/1379\n","Processed 980/1379\n","Processed 985/1379\n","Processed 990/1379\n","Processed 995/1379\n","Processed 1000/1379\n","Processed 1005/1379\n","Processed 1010/1379\n","Processed 1015/1379\n","Processed 1020/1379\n","Processed 1025/1379\n","Processed 1030/1379\n","Processed 1035/1379\n","Processed 1040/1379\n","Processed 1045/1379\n","Processed 1050/1379\n","Processed 1055/1379\n","Processed 1060/1379\n","Processed 1065/1379\n","Processed 1070/1379\n","Processed 1075/1379\n","Processed 1080/1379\n","Processed 1085/1379\n","Processed 1090/1379\n","Processed 1095/1379\n","Processed 1100/1379\n","Processed 1105/1379\n","Processed 1110/1379\n","Processed 1115/1379\n","Processed 1120/1379\n","Processed 1125/1379\n","Processed 1130/1379\n","Processed 1135/1379\n","Processed 1140/1379\n","Processed 1145/1379\n","Processed 1150/1379\n","Processed 1155/1379\n","Processed 1160/1379\n","Processed 1165/1379\n","Processed 1170/1379\n","Processed 1175/1379\n","Processed 1180/1379\n","Processed 1185/1379\n","Processed 1190/1379\n","Processed 1195/1379\n","Processed 1200/1379\n","Processed 1205/1379\n","Processed 1210/1379\n","Processed 1215/1379\n","Processed 1220/1379\n","Processed 1225/1379\n","Processed 1230/1379\n","Processed 1235/1379\n","Processed 1240/1379\n","Processed 1245/1379\n","Processed 1250/1379\n","Processed 1255/1379\n","Processed 1260/1379\n","Processed 1265/1379\n","Processed 1270/1379\n","Processed 1275/1379\n","Processed 1280/1379\n","Processed 1285/1379\n","Processed 1290/1379\n","Processed 1295/1379\n","Processed 1300/1379\n","Processed 1305/1379\n","Processed 1310/1379\n","Processed 1315/1379\n","Processed 1320/1379\n","Processed 1325/1379\n","Processed 1330/1379\n","Processed 1335/1379\n","Processed 1340/1379\n","Processed 1345/1379\n","Processed 1350/1379\n","Processed 1355/1379\n","Processed 1360/1379\n","Processed 1365/1379\n","Processed 1370/1379\n","Processed 1375/1379\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A6LKA-5MGA63","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}